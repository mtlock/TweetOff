{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained classifiers/vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Clinton_logistic_classifier.pickle', 'rb')\n",
    "C_logistic_classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Clinton_vectorizer.pickle', 'rb')\n",
    "C_vectorizer = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Trump_logistic_classifier.pickle', 'rb')\n",
    "T_logistic_classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Trump_vectorizer.pickle', 'rb')\n",
    "T_vectorizer = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load hashtag classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('Clinton_hashtag_prob.pickle', 'rb')\n",
    "Clinton_prob_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Trump_hashtag_prob.pickle', 'rb')\n",
    "Trump_prob_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_list = Clinton_prob_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all raw tweet data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = '***'\n",
    "username = '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM raw_tweet_table;\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(sql_query,con)\n",
    "#df is a dataframe with columns 'created_at','text' and 'hashtags'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#negate words between negative word and next punctuation by appending _neg\n",
    "def negation_process(tweet):\n",
    "    #add final period to ensure negation if no final punctuation\n",
    "    tweet = tweet + '.'\n",
    "    tweet = re.sub(r'\\b(?:never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)\\b[\\w\\s]+[^\\w\\s]', \n",
    "       lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1neg_\\2', match.group(0)), tweet,flags=re.IGNORECASE)\n",
    "    #return tweet[:-1] to remove added period\n",
    "    return tweet[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Porter stemming\n",
    "def stemming(tweet):\n",
    "    temp = []\n",
    "    for word in tweet.split():\n",
    "        temp.append(stemmer.stem(word.lower()))\n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#primary text processing\n",
    "def process_text(tweet_list):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweet_list:\n",
    "        tweet = re.sub(r\"(?:\\@|https?\\://|#)\\S+\", \"\", tweet)\n",
    "        tweet = tweet.replace('\\'','')\n",
    "        #negate\n",
    "        tweet = negation_process(tweet)\n",
    "        #replace non ascii characters\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "        tweet = tweet.replace('RT','')\n",
    "        tweet = tweet.replace(':','')\n",
    "        tweet = tweet.replace('+',' ')\n",
    "        tweet = tweet.replace(',','')\n",
    "        tweet = tweet.replace('.','')\n",
    "        tweet = tweet.replace('\\\"','')\n",
    "        #remove duplicate consecutive characters for standardization\n",
    "        tweet = re.sub(r'(\\S)\\1+', r'\\1', tweet)\n",
    "        #add spaces before emotive punctuation, useful for bigrams\n",
    "        tweet = tweet.replace('!',' !')\n",
    "        tweet = tweet.replace('?',' ?')\n",
    "        tweet = tweet.strip()\n",
    "        tweet = stemming(tweet)\n",
    "        processed_tweets.append(tweet)\n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(data):\n",
    "    try:\n",
    "        tweet_hashtags_split = data['hashtags'].split()\n",
    "        if len(tweet_hashtags_split) >= 1:\n",
    "            C_hashtag_prob = 1\n",
    "            T_hashtag_prob = 1\n",
    "            for tag in tweet_hashtags_split:   \n",
    "                try:\n",
    "                    C_hashtag_prob = C_hashtag_prob * Clinton_prob_dict[tag]\n",
    "                    T_hashtag_prob = T_hashtag_prob * Trump_prob_dict[tag]\n",
    "                except:\n",
    "                    pass\n",
    "            if C_hashtag_prob > T_hashtag_prob:\n",
    "                return 'C'\n",
    "            elif C_hashtag_prob < T_hashtag_prob:\n",
    "                return 'T'\n",
    "            else:\n",
    "                try:\n",
    "                    return classify_text(data['text'])\n",
    "                except:\n",
    "                    return 'skip'\n",
    "        else:\n",
    "            try:\n",
    "                return classify_text(data['text'])\n",
    "            except:\n",
    "                return 'skip'\n",
    "    except:\n",
    "        try:\n",
    "            return classify_text(data['text'])\n",
    "        except:\n",
    "            return 'skip'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hillary_names = ['clinton','hilari','she','her']#'hilari' is how all versions of 'hillary' appear after text processing\n",
    "Trump_names = ['trump','donald','he','his']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deals with tweets having multiple subjects and clauses\n",
    "def multi_subject_split(original):\n",
    "    #split on any punctuation marks besides\n",
    "    separated = re.split('[?.!,:;]',original)\n",
    "    Clinton_phrases = ''\n",
    "    Trump_phrases = ''\n",
    "    for phrase in separated:\n",
    "        phrase = process_text(phrase)\n",
    "        c=0\n",
    "        t=0\n",
    "        for word in phrase.split():\n",
    "            if word in Hillary_names:\n",
    "                c=1\n",
    "            elif word in Trump_names:\n",
    "                t=1\n",
    "        if c==1 and t==1:\n",
    "            continue\n",
    "        elif c==1 and t==0:\n",
    "            Clinton_phrases = Clinton_phrases + phrase\n",
    "        elif c==0 and t==1:\n",
    "            Trump_phrases = Trump_phrases + phrase\n",
    "        else:\n",
    "            Trump_phrases = Trump_phrases + phrase\n",
    "            Clinton_phrases = Clinton_phrases + phrase\n",
    "    Clinton_prob = C_logistic_classifier.predict_proba(C_vectorizer.transform([Clinton_phrases]))\n",
    "    Trump_prob = T_logistic_classifier.predict_proba(T_vectorizer.transform([Trump_phrases]))\n",
    "    total_prob = Clinton_prob + (np.ones(2)-Trump_prob)\n",
    "    if total_prob[0][1] >= total_prob[0][0]:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'T'           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text classifier\n",
    "def classify_text(text):\n",
    "    original = text\n",
    "    text = process_text(text)\n",
    "    c=0\n",
    "    t=0\n",
    "    for word in text.split():\n",
    "        if word in Hillary_names:\n",
    "            c=1\n",
    "        elif word in Trump_names:\n",
    "            t=1\n",
    "    if c==1 and t==1:\n",
    "        return multi_subject_split(original)\n",
    "    elif c==1 and t==0:\n",
    "        prob = C_logistic_classifier.predict_proba(C_vectorizer.transform([text]))\n",
    "        if prob[0][1] >= prob[0][0]:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'T'\n",
    "    elif c==0 and t==1:\n",
    "        prob = T_logistic_classifier.predict_proba(T_vectorizer.transform([text]))\n",
    "        if prob[0][0] >= prob[0][1]:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'T'\n",
    "    else:\n",
    "        return 'skip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run classifier and make classified column in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Classified = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(df)):\n",
    "    data = df.iloc[i]\n",
    "    Classified.append(classify(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['classified'] = Classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove rows that should be skipped\n",
    "df = df[df['classified'] != 'skip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export classified: timestamp,hashtag list and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Classified_df.pickle', 'wb')\n",
    "pickle.dump(df[['created_at','hashtags','classified']], f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
